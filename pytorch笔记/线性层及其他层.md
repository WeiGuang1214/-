### 1、正则化层 Normalization layers，加快网络训练速度

### 2、Recurrent Layers  

### 3、 Transformer层

### 4、dropout层

### 5、线性层Liner，类似于线性变换，K=x1+b，weight相当于k，bias相当于b

<img src="C:\Users\微光\AppData\Roaming\Typora\typora-user-images\image-20240822192853697.png" alt="image-20240822192853697" style="zoom:50%;" />

```python

print(output)
```
